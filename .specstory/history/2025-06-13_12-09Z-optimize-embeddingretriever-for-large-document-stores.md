<!-- Generated by SpecStory -->

# Optimize EmbeddingRetriever for large document stores (2025-06-13 12:09Z)

_**User**_

Can you review how the EmbeddingRetriever class handles batch retrieval in dense retrieval pipelines, and suggest changes to reduce memory usage and improve scalability for very large document stores (e.g., 10M+ documents)? Please include relevant file references and highlight if the embedding matrix or FAISS index loading could be optimized to support streaming or lazy loading approaches.



---

_**Assistant**_

I'm going to look into how the `EmbeddingRetriever` class handles batch retrieval in dense retrieval pipelines and suggest optimizations for memory usage and scalability with large document stores.



---

<details>
            <summary>Searched codebase "EmbeddingRetriever class batch retrieval in dense retrieval pipelines" â€¢ **20** results</summary>
        
| File | Lines |
|------|-------|
| `app/haystack/components/retrievers/in_memory/embedding_retriever.py` | L1-51 |
| `app/haystack/components/retrievers/in_memory/embedding_retriever.py` | L143-194 |
| `app/test/components/retrievers/test_in_memory_embedding_retriever.py` | L142-167 |
| `app/haystack/components/embedders/openai_document_embedder.py` | L181-217 |
| `app/haystack/components/retrievers/in_memory/embedding_retriever.py` | L51-97 |
| `app/proposals/text/5390-embedders.md` | L112-141 |
| `app/haystack/components/embedders/azure_document_embedder.py` | L183-230 |
| `app/haystack/components/retrievers/sentence_window_retriever.py` | L1-79 |
| `app/haystack/components/embedders/openai_document_embedder.py` | L1-38 |
| `app/e2e/pipelines/test_dense_doc_search.py` | L36-72 |
| `app/haystack/components/embedders/azure_document_embedder.py` | L230-255 |
| `app/haystack/components/embedders/sentence_transformers_document_embedder.py` | L204-248 |
| `app/proposals/text/4284-drop-basecomponent.md` | L834-865 |
| `app/proposals/text/5390-embedders.md` | L43-78 |
| `app/haystack/document_stores/in_memory/document_store.py` | L566-591 |
| `app/haystack/components/retrievers/in_memory/bm25_retriever.py` | L125-163 |
| `app/e2e/pipelines/test_evaluation_pipeline.py` | L1-42 |
| `app/haystack/components/retrievers/__init__.py` | L1-10 |
| `app/haystack/components/rankers/sentence_transformers_diversity.py` | L225-268 |
| `app/releasenotes/notes/embedding-instructions-4feb216cbf796678.yaml` | L1-5 |

</details>

---

I've reviewed the codebase related to the `EmbeddingRetriever` class, specifically focusing on the `InMemoryEmbeddingRetriever` in the provided search results. Let me analyze how it handles batch retrieval and suggest optimizations for memory usage and scalability with large document stores (10M+ documents).

### Analysis of Batch Retrieval in `InMemoryEmbeddingRetriever`

From the file `app/haystack/components/retrievers/in_memory/embedding_retriever.py`, the `InMemoryEmbeddingRetriever` class handles retrieval as follows:

- **Embedding Retrieval Process**: The `run` method processes a query embedding and retrieves documents based on semantic similarity. It delegates the actual retrieval to the `document_store.embedding_retrieval` method (lines 143-194). This method does not explicitly handle batching of queries or documents at the retriever level; it processes a single query embedding at a time.
- **Document Store Interaction**: The `InMemoryDocumentStore.embedding_retrieval` method (referenced in line 189) computes similarity scores between the query embedding and all document embeddings in memory, then returns the top-k most similar documents. This approach loads all document embeddings into memory for comparison, which is not scalable for very large document stores.

### Memory Usage and Scalability Issues

1. **In-Memory Storage**: The `InMemoryDocumentStore` holds all document embeddings in memory (as seen in `app/haystack/document_stores/in_memory/document_store.py`, lines 566-591). For 10M+ documents, assuming each embedding is a 768-dimensional vector (common for models like BERT), and each float is 4 bytes, this would require approximately 23 GB of RAM just for embeddings (10M * 768 * 4 bytes), not counting other document metadata or overhead.
2. **Full Similarity Computation**: The similarity computation happens across all documents in memory for each query, which is computationally expensive and memory-intensive for large datasets.
3. **No Streaming or Lazy Loading**: There is no mechanism for streaming or lazy loading of embeddings or FAISS indices. Everything is loaded into memory upfront, which is unsustainable for large-scale applications.

### Suggested Optimizations

To improve scalability and reduce memory usage, I propose the following changes:

1. **Use Disk-Based or Distributed Document Stores**:
   - Replace `InMemoryDocumentStore` with a disk-based or distributed store like `PineconeDocumentStore`, `QdrantDocumentStore`, or `ElasticsearchDocumentStore` with embedding support (referenced in `app/haystack/components/retrievers/sentence_window_retriever.py`). These stores are designed to handle large-scale data by offloading storage and computation to disk or distributed systems.
   - **Implementation**: Modify the `InMemoryEmbeddingRetriever` initialization to accept alternative document stores and ensure compatibility with their retrieval APIs.

2. **Implement FAISS or Similar Indexing with Lazy Loading**:
   - Integrate a library like FAISS (Facebook AI Similarity Search) for efficient similarity search. FAISS supports disk-based indices and lazy loading, where only necessary parts of the index are loaded into memory during queries.
   - **Implementation**: Extend `InMemoryDocumentStore` or create a new `FAISSIndexedDocumentStore` class that builds a FAISS index on disk during indexing. During retrieval, load only the relevant index shards or use memory-mapped files to minimize RAM usage. Update `embedding_retrieval` to use FAISS search methods instead of in-memory similarity computation.

3. **Batch Processing for Indexing and Retrieval**:
   - While the current retriever handles single queries, the embedding process in components like `OpenAIDocumentEmbedder` (seen in `app/haystack/components/embedders/openai_document_embedder.py`, lines 181-217) already uses batching for efficiency. Extend this concept to retrieval by processing multiple queries in batches if applicable, or batching document embeddings during indexing to build indices incrementally.
   - **Implementation**: Modify the `run` method to accept a list of query embeddings and process them in batches, reducing overhead for multiple queries.

4. **Streaming Support for Large Datasets**:
   - Implement streaming for document ingestion and retrieval to process data in chunks rather than loading everything at once.
   - **Implementation**: Add a streaming mode to the document store and retriever, where documents are processed and indexed in chunks, and only relevant chunks are loaded during retrieval based on filters or initial coarse retrieval.

5. **Embedding Matrix Optimization**:
   - Store embeddings in a compressed format (e.g., using quantization techniques supported by FAISS) to reduce memory footprint.
   - **Implementation**: During indexing, apply quantization to embeddings before storing them in the document store or FAISS index. During retrieval, ensure the similarity computation accounts for the quantization method used.

### Specific File References for Changes

- **Primary File to Modify**: `app/haystack/components/retrievers/in_memory/embedding_retriever.py` - Update the `run` method to support batch queries and integrate with a scalable document store.
- **Document Store Enhancements**: `app/haystack/document_stores/in_memory/document_store.py` - Extend or create a new class for FAISS or disk-based storage with lazy loading capabilities.
- **Indexing Pipeline**: Refer to `app/e2e/pipelines/test_dense_doc_search.py` (lines 18-72) to ensure indexing pipelines can handle streaming and batching for large datasets.

### Conclusion

The current implementation of `InMemoryEmbeddingRetriever` is not suitable for very large document stores due to its in-memory nature and lack of lazy loading or streaming. By transitioning to disk-based or distributed document stores, integrating FAISS with lazy loading, and implementing batch processing and streaming, memory usage can be significantly reduced, and scalability improved for handling 10M+ documents.

Would you like me to propose specific code edits to start implementing these optimizations, focusing on integrating FAISS or switching to a different document store?

---

